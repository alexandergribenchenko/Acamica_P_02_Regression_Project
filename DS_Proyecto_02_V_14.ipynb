{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "DS_Proyecto_02.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandergribenchenko/Acamica_Proyecto_02/blob/main/DS_Proyecto_02_V_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAMLpLgdGJ5W"
      },
      "source": [
        "# **Proyecto 02 - Ingeniería de features, Modelos avanzados e Interpretación de modelos** (Armado) V_14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovEgj74GJ5X"
      },
      "source": [
        "### (Acámica - DS online 40 - Proyecto 02 - Alexander Ortega)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxpYDd1mEb4k"
      },
      "source": [
        "# **Alcance y objeto del proyecto:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MeSr6lLEnBg"
      },
      "source": [
        "El presente proyecto tiene como finalidad exhibir e implementar un conjunto de herramientas analíticas y predictivas propias de problemas del aprendizaje supervisado, más especificamante de problemas de regresión, empleando como contexto específico el estudio de los precios de los inmuebles de la Ciudad de Buenos Aires entre 2019 y enero de 2020, con el fin de generar y evaluar modelos que permitan hacer su predicción. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEBtnt9FFan"
      },
      "source": [
        "##### ---> Librerías a emplear en el proyecto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9M2TjbgQkfh"
      },
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtObtnd0QaSw"
      },
      "source": [
        "!pip install --upgrade xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6jnKm8jGJ5Y"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance, plot_tree\n",
        "\n",
        "import pickle\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcQnnKLzGJ5a"
      },
      "source": [
        "# **Etapa Preliminar.** Preparación del dataset de análisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfAihOqRVK82"
      },
      "source": [
        "En la presente etapa,  que dividiremos en  4 pasos, recopilaremos y cargaremos la información primaria para el estudio, exploramos el dataset cargado (raw) para entender mejor cada una de las variables y entender con ello qué tipo de procesamiento deberíamos aplicar, eliminaremos los atributos que hayamos encontrado que no sean relevantes para el modelo y filtraremos respecto a las categorías de interés, y finalmente definiremos el dataset base que será el que empleado de manera posterior en la etapa de Transformación de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUIsDsfGUnjl"
      },
      "source": [
        "## **EP. 1.1.** Recopilación y descarga de la información primaria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ced5edUpTK"
      },
      "source": [
        "La información base para el desarrollo del siguiente proyecto fue obtenida de la [página de la división de datos de Properati](https://www.properati.com.co/data/), una plataforma web enfocada en el mercado inmobiliario de 4 países de Latinoamérica: Argentina, Colombia, Perú y Ecuador. En específico en la sección de Datasets del enlace descrito encontramos el enlace de descarga directa  en formato CSV para cada uno de los países.\n",
        "\n",
        "Para efectos del desarrollo del proyecto generamos únicamente la descarga del dataset de Argentina y tomamos en cuenta únicamente las propiedades publicadas entre 2019 y enero de 2020, ubicadas al interior del Gran Buenos Aires. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjeH8bQWlKP"
      },
      "source": [
        "## **EP. 1.2.** Exploración del dataset (raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXeWGpz8Wqvv"
      },
      "source": [
        "Iniciaremos cargando y evaluando la información general del dataset descargado (Argentina, 2019):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fevtqAxiLqLj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur_CiW6ZLzf6"
      },
      "source": [
        "path_data = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/DS_Proyecto_01_Datos_Properati.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G571lUV6GJ5a"
      },
      "source": [
        "df = pd.read_csv(path_data, sep =',')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9403w-ZGJ5b"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmUsIwTCXWep"
      },
      "source": [
        "Corroboramos que el dataset cuenta con los siguientes 19 atributos:\n",
        "\n",
        "- **start_date:** fecha en formato año-mes-día en que inició la publicación.\n",
        "\n",
        "- **end_date:** fecha en formato año-mes-día en que finalizó la publicación.\n",
        "\n",
        "- **created_on:** fecha en formato año-mes-día en que se creó la publicación.\n",
        "\n",
        "- **lat:** latitud de la ubicación.\n",
        "\n",
        "- **lon:** longitud de la ubiucación.\n",
        "\n",
        "- **l1:** país del inmueble.\n",
        "\n",
        "- **l2:** zona del Gran Buenos Aires donde esta ubicado.\n",
        "\n",
        "- **l3:** barrio donde esta ubicado.\n",
        "\n",
        "- **rooms:** cantidad de espacios independientes.\n",
        "\n",
        "- **bedrooms:** cantidad de habitaciones.\n",
        "\n",
        "- **bathrooms:** cantidad de baños.\n",
        "\n",
        "- **surface_total:** superficie total.\n",
        "\n",
        "- **surface_covered:** superficie construida.\n",
        "\n",
        "- **price:** precio (este es nuestro target).\n",
        "\n",
        "- **currency:** moneda en que esta dado el precio.\n",
        "\n",
        "- **title:** titulo de la publicación.\n",
        "\n",
        "- **description:** descripción del inmueble.\n",
        "\n",
        "- **property_type:** tipo de propiedad.\n",
        "\n",
        "- **operation_type:** tipo de operación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpDnRROl553H"
      },
      "source": [
        "Exploraremos ahora con mayor detalle cada uno de los atributos para determinar cuáles de ellos nos son útiles, de cuales de ellos debemos prescindir porque no representan información relevante en nuestro contexto y cuáles deben ser transformados de manera inicial para que tengan una representación más limpia y homogénea en el dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbfCKbkXWZN"
      },
      "source": [
        "### **---> Exploración variable start_date, end_date, y created_on:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i251wHZZZuSr"
      },
      "source": [
        "df.start_date = pd.to_datetime(df.start_date)\n",
        "print(df.start_date.min().date())\n",
        "print(df.start_date.max().date())\n",
        "df_temp = df.start_date.value_counts().to_frame().reset_index().sort_values(by=['index']).reset_index(drop=True)\n",
        "df_temp=df_temp.groupby([pd.Grouper(key= 'index', freq='MS')])\\\n",
        "        .sum().reset_index()\n",
        "df_temp['index'] = df_temp['index'].apply(lambda x: str(x.date())[0:7])\n",
        "df_temp.columns=['Mes','Cantidad publicaciones']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naKxflRU77BX"
      },
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "sns.barplot(data=df_temp, x=\"Mes\", y=\"Cantidad publicaciones\")\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xomVuInrKOoD"
      },
      "source": [
        "Aunque vemos que hay un comportamiento diferencial entre la cantidad de publicaciones en los diferentes meses del año, no centraremos nuestro estudio en dicho comportamiento. Si bien el precio de los inmuebles puede fluctuar dependiendo de la fecha de publicación, este comportamiento temporal es un comportamiento que se sugeriría estudiar con mayor detalle en el caso que se encuentre relevante para un segmento de inmuebles particular. \n",
        "\n",
        "Para nuestro caso específico prescindiremos de las variables asociadas a las fechas y asumiremos que las fechas tanto de inicio, finalización y creación de la publicación, no son relevantes frente a las restantes características de los inmuebles.\n",
        "\n",
        "**Procesamiento a aplicar:** las variables que serán descartadas en el dataframe de analisis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ics78OULZbzM"
      },
      "source": [
        "### **---> Exploración variables lat y lon:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXz1ileLZbzN"
      },
      "source": [
        "df[['lat','lon']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjEp0HbzMn1L"
      },
      "source": [
        "Aunque observamos que la distribución de latitudes y longitudes pueden presentar algunos valores atípicos en sus maximos y minimos, vemos acorde al percentil 75% y a su valor medio, que en general las latitudes y longitudes corresponden con las de la ciudad de buenos aires  (valores esperados próximos a: latitud 34.36 y longitud 58.26). \n",
        "\n",
        "Estas variables las tendremos en cuenta pues corresponden con geolocalizaciones, que en conjunto con otras variables de ubicación pueden presentar una influencia significativa en el precio de los inmuebles. \n",
        "\n",
        "**Procesamiento a aplicar:** las variables son de tipo numérico continuo. Se evaluarán y descartaran outliers y se escalarán sus datos. Los datos con valores faltantes no se imputarán, pues corresponden con a una ubicación muy específica para cada inmueble que no sería sensato obtener a partir de una imputación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_IYQThZcFJ"
      },
      "source": [
        "### **---> Exploración variables l1, l2 y l3:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H0RaeTAZcFK"
      },
      "source": [
        "df.l1.value_counts(dropna=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw9apCZKThtO"
      },
      "source": [
        "La variable l1 será descartada del total pues todos los inmuebles se encuentran en Argentina por lo cual no agrega valor alguno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrooF-LgZcMg"
      },
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "print(round(100*df.l2.value_counts(dropna=False, normalize=True),2))\n",
        "sns.countplot(data = df, y = 'l2', order = df['l2'].value_counts().index, palette='cool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPRywBHqTmC1"
      },
      "source": [
        "Dado que acorde a la distribución de la variable l2 la zona más representativa en relación con la cantidad de registros es Capital Federal (63% de los registros), filtraremos nuestro dataset para tener únicamente en cuenta los de esta Zona y así tener resultados comparativos de los modelos con los presentados en el primer proyecto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFL76b6ZcTM"
      },
      "source": [
        "plt.figure(figsize = (15,20))\n",
        "print(round(100*df.l3.value_counts(dropna=False, normalize=True),2))\n",
        "sns.countplot(data = df, y = 'l3', order = df['l3'].value_counts().index, palette='cool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AafJ8gVTVjA2"
      },
      "source": [
        "Aunque respecto a la variable L3 se encuentra una distribución con barrios con cantidades significativas de registros (Palermo, Tigre, Almagro) y otros barrios con cantidades muy pequeñas (Catalinas, Marcos Paz), dejaremos todos los barrios para tener la totalidad de Capital federal para nuestro análisis. Además la localización del inmueble acorde al barrio puede ser una de las propiedades más sensibles para establecer su precio y queremos observar esta influencia en los modelos a evaluar.\n",
        "\n",
        "**Procesamiento a aplicar:** las variables que son de tipo categórico nominal, por tanto para poder incluirlos en los modelos aplicaremos one hot encoding. Adicional a ello los valores faltantes podrán imputarse a partir de las variables restantes que se consideren de mayor interés (latitud y longitud de manera más específica para esta variable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEX4cdmZcZi"
      },
      "source": [
        "### **---> Exploración variables rooms, bedrooms, bathrooms:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNWQCEv8ZcZi"
      },
      "source": [
        "df[['rooms','bedrooms', 'bathrooms']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIgYM7Kpaelz"
      },
      "source": [
        "Acorde a la distribución de las variables encontramos fijándonos en el valor de los percentiles que por lo menos el 75% de las propiedades corresponden a inmuebles que tienen como máximo 4 espacios, 3 habitaciones y 2 baños. Esto corresponde en general con propiedades que seguramente serán del tipo Departamento, Casa o PH. \n",
        "\n",
        "Mantendremos todas las variables, pues en conjunto con la ubicación y la superficie pueden ser las variables más relevantes al determinar el precio de los inmuebles.\n",
        "\n",
        "**Procesamiento a aplicar:** las variables que son de tipo numérico discreto y esto hace que se puedan tener 2 aproximaciones en su transformación. Por una parte al tratarse de variables numéricas podemos establecer outliers respecto a su distribución y podemos normalizarlas, y por otra parte, dado que los grupos en los cuales se distribuyen no son muchas pueden considerarse también como variables categóricas de tipo ordinal con lo cual podría aplicarse label encoding para incluirlas en el modelo. La elección de la bondad de un camino u otro se dará al momento de evaluar los modelos. Adicional a ello los valores faltantes podrán imputarse a partir de las variables restantes que se consideren de mayor interés (quizá superficie, tipo de propiedad y las restantes rooms, bedrooms y bathroms que se conozcan).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJr_1s6fZcx9"
      },
      "source": [
        "### **---> Exploración variables surface_total  y surface covered:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CE3wtULZcx-"
      },
      "source": [
        "df[['surface_total','surface_covered']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsUtiaXShdmq"
      },
      "source": [
        "Acorde a la distribución de las variables encontramos que son quizá las presentan un comportamiento menos homogéneo. Puede observarse como la desviación estándar es muy superior a los valores medios, y como entre los percentiles 75% y los valores máximos, o como entre el percentil 25% y los valores mínimos encontramos grandes brechas.\n",
        "\n",
        "Dado que el tipo de propiedades que evaluaremos corresponden a propiedades de tipo del tipo Departamento, Casa o PH, en las cuales la mayoría la superficie construido abarca casi la totalidad de la perficie cubierta, eliminaremos la variable de superficie cubierta para que no presenten colinealidad, lo que puede hacer que algunos modelos (como el regresor lineal, por ejemplo) presenten comportamientos anómalos por no lograr discriminar de manera diferenciada el aporte que están teniendo cada una de las dos variables.\n",
        "\n",
        "Sin embargo la variable que mantendremos, superficie total, será una de las variables más sensible al momento de estimar el precio de los inmuebles.\n",
        "\n",
        "**Procesamiento a aplicar:** las variables son de tipo numérico continuo por lo cual se generará por un aparte eliminación de outliers y por otra escalado de los datos que acorde a la distribución puede ser de tipo logarítmico (dada la larga cola de la distribución, gran distancia entre el percentil 75 y los valores máximos). Adicional a ello los valores faltantes podrán imputarse a partir de las variables restantes que se consideren de mayor interés (cantidad de rooms, bedroooms y el otro valor de superficie si se conoce)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhc38s8ZdIo"
      },
      "source": [
        "### **---> Exploración variable price:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v4hTNkXZdIp"
      },
      "source": [
        "df[['price']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEujNY-ejd9_"
      },
      "source": [
        "La variable precio presenta un comportamiento similar a la variable superficie en cuanto a las distribuciones de percentiles, mínimos, máximos y desviaciones estándar. Por esta razón presentará un tratamiento similar. Es de resaltar además que esta es nuestra variable objetivo de estudio por lo cual debe mantenerse de manera obligatoria.\n",
        "\n",
        "**Procesamiento a aplicar:** la variables son de tipo numérico continuo por lo cual se generará por un aparte eliminación de outliers y por otra escalado de los datos que acorde a la distribución puede ser de tipo logarítmico (dada la larga cola de la distribución, gran distancia entre el percentil 75 y los valores máximos). Adicional a ello los valores faltantes podrán imputarse a partir de las variables restantes que se consideren de mayor interés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKv_gtI5ZdPu"
      },
      "source": [
        "### **---> Exploración variable currency:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhmBbbwzZdPv"
      },
      "source": [
        "df.currency.value_counts(dropna=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBkhn9L0lwWG"
      },
      "source": [
        "La variable currency será descartada del total pues todos los inmuebles corresponden a dolares por lo cual no agrega valor alguno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSqseAsRZdXC"
      },
      "source": [
        "### **---> Exploración variables title y description:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUNDc53xZdXC"
      },
      "source": [
        "df.title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AJreAvSno5K"
      },
      "source": [
        "df.description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_MjSamLnFIV"
      },
      "source": [
        "Aunque la descripción del inmueble y el titulo del anuncio puedan ser elementos de interés al estudiarse con herramientas de procesamiento de lenguaje natural (NLP), este enfoque excede el alcance que hemos dado al presente proyecto por lo cual será una variable que descartaremos de antemano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldbGS_MZZdlc"
      },
      "source": [
        "### **---> Exploración variable property_type:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9IvCDQVZdlc"
      },
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "print(round(100*df.property_type.value_counts(dropna=False, normalize=True),2))\n",
        "sns.countplot(data = df, y = 'property_type', order = df['property_type'].value_counts().index, palette='cool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5MwtLlGp_5j"
      },
      "source": [
        "Dado que acorde a la distribución de la variable tipo de inmueble las  categorías más representativas son Departamento, Casa y PH (en conjunto representan más del 90% de las propiedades), filtraremos nuestro dataset para tener únicamente en cuenta estas 3 categorías y así tener información comparativa de los modelos con los presentados en el primer proyecto. \n",
        "\n",
        "**Procesamiento a aplicar:** la variable es de tipo categórico nominal, por tanto para poder incluirla en los modelos aplicaremos one hot encoding. Adicional a ello los valores faltantes podrán imputarse a partir de las variables restantes que se consideren de mayor interés (superficie y cantidad de baños, espacios y habitaciones)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34v6tYSxe-Ei"
      },
      "source": [
        "### **---> Exploración variable operation_type:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNeC8A_gPdv"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDjaEqW9e-Ej"
      },
      "source": [
        "df.operation_type.value_counts(dropna=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98u3bJv8e-Ek"
      },
      "source": [
        "La variable operation_type será descartada del total pues todos los inmuebles se encuentran en el dataset son de tipo venta, lo cual no agrega valor alguno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGixeA-KsWEM"
      },
      "source": [
        "## **EP. 1.3.** Eliminación de variables y filtrado de categorias de interes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PiZwg-Zufsu"
      },
      "source": [
        "### **---> Eliminación de variables:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oGM1E6wvO07"
      },
      "source": [
        "Acorde a lo discutido en el numeral anterior, descartamos del total 3 tipos de variables que no resultan útiles para nuestro caso de estudio: variables que presentan atributos asociados a fechas, variables que presenta un único valor y variables con textos descriptivos. Estas variables las eliminamos en esta sección de manera definitiva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I6FgwC4tE-E"
      },
      "source": [
        "# Atributos asociados a fechas\n",
        "list_descartar_dates= ['start_date','end_date','created_on']\n",
        "# Atributos con un único valor\n",
        "list_descartar_uni_val= ['l1','currency','operation_type']\n",
        "# Atributo con texto descriptivo\n",
        "list_descartar_text_vdesc= ['title','description']\n",
        "# Totalidad de los atributos a descartar\n",
        "list_descartar = list_descartar_dates + list_descartar_uni_val + list_descartar_text_vdesc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQOb7ie3tFAl"
      },
      "source": [
        "# Creamos nuestro dataframe base a partir de os valores ya filtrados\n",
        "df_base = df.drop(list_descartar,axis=1)\n",
        "df_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktVgHTt4vYe2"
      },
      "source": [
        "### **---> Filtrado de categorías de interes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfLRzdFal0Av"
      },
      "source": [
        "Acorde a lo discutido en el numeral anterior, tomaremos en cuenta únicamente las propiedades ubicadas en Capital Federal (63% del total de los registros), y las propiedades de tipo Departamento, Casa y PH (en conjunto representan el 90% del total de las propiedades). Esta muestra también nos permitirá tener un conjunto de datos comparativo respecto al primer proyecto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59FSUxtStFCt"
      },
      "source": [
        "# Filtrado de propiedades ubicadas en Capital Federal\n",
        "df_base = df_base[(df_base.l2 == 'Capital Federal')]\n",
        "df_base = df_base.drop(['l2'],axis=1)\n",
        "# Filtrado de pripiedades de tipo Departamento, Casa y PH\n",
        "df_base=df_base[df_base.property_type.isin(['Departamento','Casa','PH'])]\n",
        "df_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nWKzqfXriVU"
      },
      "source": [
        "## **EP. 1.4.** Definición del dataset base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW7NwZMlrkT2"
      },
      "source": [
        "Finalmente reorganizamos las variables y obtenemos el dataframe base que emplearemos en la siguiente etapa de transformación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK-v0ffTsn0S"
      },
      "source": [
        "df_base = df_base.reset_index(drop=True)\n",
        "df_base = df_base[['l3',  'property_type', 'lat', 'lon', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
        "       'surface_covered', 'price']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXpnae4JtGo9"
      },
      "source": [
        "df_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7LNOj9Du6P5"
      },
      "source": [
        "# **Parte A.** Trasformación de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3AQUeDBvexi"
      },
      "source": [
        "df_A = df_base.copy()\n",
        "df_A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1geMRVG7xEOv"
      },
      "source": [
        "## **A. 01.** Detección y eliminación de Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Btr4inE3s07"
      },
      "source": [
        "La detección y eliminación de outliers se da para registros de tipo numérico ya sean de tipo discreto o continuo. En específico existen 2 tipos de criterios generales con los cuales se pueden detectar valores outliers en una distribución: el criterio de los rangos intercuartílicos y el criterio de los 3 sigmas. Ambos criterios son empleados de manera conjunta en esta sección para determinar los que valores consideraremos outliers y filtrarlos del dataset de análisis que emplearemos para nuestros modelos de predicción, esto hará que nuestros modelos se centren más en el comportamiento de los valores que se consideran representativos dentro de cada distribución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE6FIL2Hxt6F"
      },
      "source": [
        "### ---> Selección de features a aplicar detección y eliminación de outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9eNWsb4Ztb"
      },
      "source": [
        "Inicialmente definimos las columnas sobre las cuales haremos el proceso de  detección y eliminación de outliers. Como fue mencionado estas se generan sobre las columnas de tipo numérico:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv6yLLDEtIp3"
      },
      "source": [
        "col_outliers=['lat', 'lon', 'rooms', 'bedrooms', 'bathrooms',\n",
        "       'surface_total', 'surface_covered', 'price']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz_KWn_5yohQ"
      },
      "source": [
        "### ---> Definición de funciones que nos permiten detectar outliers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq4TpXiK6XZc"
      },
      "source": [
        "En esta sección se presentan de manera específica 4 funciones que nos permiten detectar outliers: \n",
        "1. **Indx_outlier_rang_int:** la función implementa el criterio del rango intercuartílico para entregarnos como salida el conjunto de índices que se consideran outliers a partir de una serie que recibe como entrada.\n",
        "2. **Indx_outlier_3_sig:** la función implementa el criterio los 3 sigmas para entregarnos como salida el conjunto de índices que se consideran outliers a partir de una serie que recibe como entrada.\n",
        "3. **indx_outlier_mix:** la función implementa conjuntamente el criterio del rango intercuartílico y de los 3 sigmas para entregarnos como salida el conjunto de índices que se consideran outliers a partir de una serie que recibe como entrada.\n",
        "4. **Indx_outlier_mix_df:** la función implementa conjuntamente el criterio del rango intercuartílico y de los 3 sigmas para entregarnos como salida el conjunto de índices que se consideran outliers a partir de un dataframe y una lista de columnas que recibe como entrada. Las columnas corresponden a las columnas en las cuales se detecten los valores outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkkVVK_vGJ52"
      },
      "source": [
        "# Definimos una función que retorna los indices de los registros que se consideran outlier\n",
        "# (Bajo el criterio basado en el rango intercuartilico)\n",
        "# Entrada: la serie de la que deseamos encontrar outlayers\n",
        "# Salida: Los indices que de los outliers en la serie\n",
        "def indx_outlier_rang_int(series):\n",
        "    q75=series.quantile(0.75)\n",
        "    q25=series.quantile(0.25)\n",
        "    iqr = q75 - q25\n",
        "    minimo = q25 - 1.5*iqr\n",
        "    maximo = q75 + 1.5*iqr\n",
        "    mascara_outliers = (series < minimo) | (series > maximo)\n",
        "    return list(series[mascara_outliers].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bto6QZoqGJ54"
      },
      "source": [
        "# Definimos una función que retorna los indices de los registros que se consideran outliers\n",
        "# (Bajo el criterio de las tres sigmas)\n",
        "# Entrada: la serie de la que deseamos encontrar outliers\n",
        "# Salida: Los indices que de los outliers en la serie\n",
        "def indx_outlier_3_sig(series):\n",
        "    valor_medio = series.mean()\n",
        "    std = series.std()\n",
        "    minimo = valor_medio - 3*std\n",
        "    maximo = valor_medio + 3*std\n",
        "    mascara_outliers = (series < minimo) | (series > maximo)\n",
        "    return list(series[mascara_outliers].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngOZDlV_GJ55"
      },
      "source": [
        "# Definimos una función que retorna los indices de los registros que se consideran outliers\n",
        "# (Empleando simultaneamento los criterios de rango intercuartilico y de los 3 sigmas)\n",
        "# Entrada: la serie de la que deseamos encontrar outliers\n",
        "# Salida: Los indices que de los outliers en la serie\n",
        "def indx_outlier_mix(series):\n",
        "    return sorted(list(set(indx_outlier_rang_int(series)+indx_outlier_3_sig(series))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtY8RvK3GJ55"
      },
      "source": [
        "# Definimos una función que nos entrega la totalidad de los indices de outliers teniendo en cuenta varias columnas\n",
        "# Entradas: dataframe del que deseamos encontrar los outliers\n",
        "# Salidas: columnas del dataframe de las que nos interesa que haga el analisis de outliers\n",
        "def indx_outlier_mix_df(df,col_outliers):\n",
        "    indx_total=[]\n",
        "    for col in col_outliers:\n",
        "        indx = indx_outlier_mix(df[col])\n",
        "        indx_total=indx_total+indx\n",
        "    return sorted(list(set(indx_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgGiGYL6_gk"
      },
      "source": [
        "### ---> Dataframe procesado sin outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Xriscu8UAQ"
      },
      "source": [
        "A partir de las funciones definidas procedemos a encontrar y a eliminar los índices que se consideran outliers en nuestro dataframe de estudio (en nuestro caso específico se encontro que el porcentaje total de valores atipicos encontrados es de 13.52%). Este dataframe sera el que emplearemos como entrada en el siguiente proceso de transformación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vz527IVxQB2"
      },
      "source": [
        "indx_outlier_total = indx_outlier_mix_df(df_A,col_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMcB3N_8xP_k"
      },
      "source": [
        "# Calculamos el porcentaje de valores del dataframe que se consideran outliers\n",
        "round(100*len(indx_outlier_total)/len(df_A),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptlhu83DxP80"
      },
      "source": [
        "# Finalmente obtendriamos el dataframe filtrado para sin los valores que consideramos outliers\n",
        "df_A_no_outliers= df_base.drop(indx_outlier_total).reset_index(drop=True)\n",
        "df_A_no_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBn86xwD90Al"
      },
      "source": [
        "## **A. 02.** Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kpp9Zn90Ay"
      },
      "source": [
        "El proceso de encoding se da para atributos de tipo categórico que sean nominales u ordinales. En específico para los atributos de tipo categórico nominal se presenta el proceso de *one hot encoding*, y para los atributos de tipo categórico ordinal se puede presentar tanto *one hot encoding* cómo *label encoding* según sea útil en cada caso de estudio. Dado que nuestro dataset cuenta únicamente con atributos de tipo categórico nominal solo implementaremos el proceso de *one hot encoding*. Este proceso es obligatorio, pues los modelos reciben únicamente como entradas variables de  variables de tipo numérico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xj8dxnW90Ay"
      },
      "source": [
        "### ---> Selección de features a aplicar encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgxoiLm90Ay"
      },
      "source": [
        "Inicialmente definimos las columnas sobre las cuales haremos el proceso de encoding. Como fue mencionado estas se generan sobre las columnas de tipo categórico:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK1wgFJA90Ay"
      },
      "source": [
        "col_encoding=['property_type', 'l3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8skMCBdF90Ay"
      },
      "source": [
        "### ---> Definición de funciones que nos permiten generar el encoding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNTjIGl90Ay"
      },
      "source": [
        "Para desarrollar el proceso de one hot encoding la librería **pandas** cuenta con la función **get_dummies** que nos permite implementar de manera directa el one hot encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gjB0s2kDgNO"
      },
      "source": [
        "### ---> Dataframe procesado con encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WlMc05WDgNZ"
      },
      "source": [
        "A partir de la función descrita obtenemos el dataframe con el encoding implementado. Este dataframe sera el que emplearemos como entrada en el siguiente proceso de transformación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jmA2-DrxPxy"
      },
      "source": [
        "df_A_encode = pd.get_dummies(df_A_no_outliers, columns=col_encoding, drop_first=False)\n",
        "df_A_encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_38DHplOFDQd"
      },
      "source": [
        "## **A. 03.** Imputación de valores faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n3ymDnEFDQm"
      },
      "source": [
        "Al momento de presentarse registros con parámetros nulos existen múltiples técnicas para tratarlos, entre estos 2 principales: la eliminación de los valores o su imputación. Cada uno acorde al problema específico debe implementarse a criterio evaluando los pros y contras que conlleva su implementación. El proceso de imputación o eliminación de valores faltantes es obligatorio, pues los modelos no reciben como entrada valores nulos. En nuestro caso específico describiremos cuando implementaremos uno u otro para cada parámetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTfv6_wOFjIF"
      },
      "source": [
        "# Observamos inicialmente que parametros presentan valores nulos\n",
        "df_A_no_outliers.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOeXE1fzFDQm"
      },
      "source": [
        "### ---> Registros que no se imputaran (se eliminaran si son nulos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0k-K4WNOPlp"
      },
      "source": [
        "En nuestra implementación definiremos un conjunto de casos específicos en los cuales no imputamos los valores sino que los eliminaremos de antemano, esto dado que la imputación puede generar valores que carezcan de sentido,  siendo finalmente poco fiable imputarlos a partir de otros parámetros. Estos seran los casos para los cuales no imputaremos sino que eliminaremos los registros: \n",
        "- **Registros que presentan valores nulos en las variables lat o lon:** no los imputaremos pues son variables completamente específicas e identitarias para cada inmueble. \n",
        "- **Registros que presentan valores nulos en las surface_total y surface_covered:** no los imputaremos pues la superficie es un parámetro determinante en relación con el precio. Si no conocemos ni la superficie construida ni la superficie total, estimar un valor de área a partir de otros parámetros puede ser poco objetivo. Ahora, en el caso que se conozca uno de los 2 puede resultar útil obtenerlo a partir del otro.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4trgwGbCxPu7"
      },
      "source": [
        "# Indices de los registros a eliminar por lat o lon nula\n",
        "indx_to_drop_01 = df_A_encode[(df_A_encode.lat.isnull()==True)|(df_A_encode.lon.isnull()==True)].index\n",
        "# Indices de los registros a eliminar por surface_total o surface_covered nula\n",
        "indx_to_drop_02 = df_A_encode[(df_A_encode.surface_total.isnull()==True)&(df_A_encode.surface_covered.isnull()==True)].index\n",
        "# Indices totales a dropear\n",
        "indx_to_drop_total = indx_to_drop_01 | indx_to_drop_02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IbJFFW-9Ufh"
      },
      "source": [
        "df_A_droped = df_A_encode.drop(indx_to_drop_total).reset_index(drop=True)\n",
        "df_A_droped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35uNhAEiFgtd"
      },
      "source": [
        "### ---> Registros que se imputaran a partir de otros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60NabNksOH19"
      },
      "source": [
        "Existen múltiples maneras de imputar registros en el caso en que estimamos que otros parámetros son útiles para estimar los valores faltantes:imputación mediante valores medios, medianas, modas, diversos tipos de regresores, entre otros. En nuestro caso especifico implementaremos una función que hará uso del imputer basado en KNN de ScikitLearn. Esta función nos permitirá obtener información a partir de un dataframe de entrada y de una lista de columnas, el mismo dataframe pero ahora con las columnas elegidas ya imputadas. Así podremos tener la libertad de elegir qué variables en especifico elegir para hacer la imputación de las restantes.\n",
        "\n",
        "Con dicha función imputamos 2 tipos de conjuntos valores en específico: \n",
        "- **Valores de surface_total y surface_covered:** los registros en los cuales se conoce un tipo de superficie y no la otra las imputaremos entre sí. \n",
        "- **Valores de bathrooms:** imputaremos estos registros nulos a partir de las variables rooms y bedrooms. Lo lógico es que estas dos variables asociadas a la cantidad de espacios distribuidos en viviendas tengan relación con la variable bathrooms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzA22OL3LFvX"
      },
      "source": [
        "# Inicialmente evaluamos las columnas que presentan valores a imputar\n",
        "df_A_droped[['lat', 'lon', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
        "       'surface_covered', 'price']].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S80N4SxpxPcJ"
      },
      "source": [
        "# Función que nos permite hacer la imputación de columnas especificas de un dataframe\n",
        "def fun_knn_imputer(df,columns):\n",
        "    df_imputed=df.copy()\n",
        "    knn_imputer = KNNImputer(n_neighbors=10, weights=\"distance\")\n",
        "    array_imputed = knn_imputer.fit_transform(df[columns])\n",
        "    df_imputed_columns = pd.DataFrame(data=array_imputed, columns=df[columns].columns, index=df[columns].index)\n",
        "    df_imputed[columns]=df_imputed_columns\n",
        "    return df_imputed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzsaNCRhKLHO"
      },
      "source": [
        "# Dataframe imputado para las variables de superficie\n",
        "df_A_imputed = fun_knn_imputer(df_A_droped,['surface_total','surface_covered'])\n",
        "# Dataframe imputado para la variable de bathrooms\n",
        "df_A_imputed = fun_knn_imputer(df_A_imputed,['rooms', 'bedrooms', 'bathrooms'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUi0Ta51QRPM"
      },
      "source": [
        "# Corroboramos que no existen ahora varibles nulas en el dataset\n",
        "df_A_imputed[['lat', 'lon', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
        "       'surface_covered', 'price']].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXg_JQO-Rhu6"
      },
      "source": [
        "## **A. 04.** Escalado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeK4IuOWRhvD"
      },
      "source": [
        "De la misma manera que en la sección anterior existen múltiples maneras de escalar los valores de nuestros features: standart scaler, normalización, min-max scaler, entre otros. En nuestro caso especifico implementaremos dos funciones que nos permitirán implementar escaladores basados en  función de los escaladores  standart scaler de ScikitLearn, y por otra parte una función de escalado logarítmico implementado de manera propia. El escalado es un proceso determinante pues estará relacionado en varios modelos con la importancia que puede tener cada feature dentro de la predicción. El escalado permitirá tener una importancia relativa para cada uno de los features involucrados. El dataframe se escalará de manera conjunta con los 3 escaladores.\n",
        " \n",
        "Cada uno de los tipos de escaladores se emplearán de la siguiente manera:\n",
        "- **Escalador logarítmico:** cuando la distribución de cierta variable presenta una larga cola el escalador logarítmico resulta útil. Esto permite al modelo discriminar de mejor manera en relación con este parámetro en específico. Estos tipos de distribuciones los podemos encontrar en las variables surface_total y surface_covered a los cuales los aplicaremos.\n",
        "- **Escalador basado en standard scaler:** Lo aplicaremos a todas las variables numéricas del dataset.\n",
        "- **Escalador basado en minmax scaler:** Lo aplicaremos a todas las variables numéricas del dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHE3w0i2KLLy"
      },
      "source": [
        "# Función que me permite escalar logaritmicamente\n",
        "# Entradas: df (dataframe que quiero escalar, col (lista con las columnas que deseo escalar))\n",
        "# Salida: dataframe escalado en las columnas que se pidio (mantiene indices de filas y columnas)\n",
        "def fun_scaler_logaritmic(df,columns):\n",
        "    df_scaled=df.copy()\n",
        "    df_scaled[columns]=df_scaled[columns].applymap(lambda x: np.log(x+1))\n",
        "    return df_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMKKfg9MXuCv"
      },
      "source": [
        "# Función que me permite escalar empleando el MinMaxScaler\n",
        "# Entradas: df (dataframe que quiero escalar, col (lista con las columnas que deseo escalar))\n",
        "# Salida: dataframe escalado en las columnas que se pidio (mantiene indices de filas y columnas)\n",
        "def fun_scaler_standar(df,columns):\n",
        "    df_scaled=df.copy()\n",
        "    std_scaler = StandardScaler()\n",
        "    array_scaled=std_scaler.fit_transform(df[columns])\n",
        "    df_scaled_columns = pd.DataFrame(data=array_scaled, columns=df[columns].columns, index=df[columns].index)\n",
        "    df_scaled[columns]=df_scaled_columns\n",
        "    return df_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9keY1YfYJZt"
      },
      "source": [
        "# Función que me permite escalar empleando el StandarScaler\n",
        "# Entradas: df (dataframe que quiero escalar, col (lista con las columnas que deseo escalar))\n",
        "# Salida: dataframe escalado en las columnas que se pidio (mantiene indices de filas y columnas)\n",
        "def fun_scaler_minmax(df,columns):\n",
        "    df_scaled=df.copy()\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "    array_scaled=minmax_scaler.fit_transform(df[columns])\n",
        "    df_scaled_columns = pd.DataFrame(data=array_scaled, columns=df[columns].columns, index=df[columns].index)\n",
        "    df_scaled[columns]=df_scaled_columns\n",
        "    return df_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1p-QcLvY-w8"
      },
      "source": [
        "# Dataframe escalado con el escalador logarítmico\n",
        "df_A_scaled = fun_scaler_logaritmic(df_A_imputed,['surface_total','surface_covered'])\n",
        "# Dataframe escalado con el standard_scaler\n",
        "df_A_scaled = fun_scaler_standar(df_A_scaled,['lat', 'lon', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
        "       'surface_covered'])\n",
        "# Dataframe escalado con el escalador minmax_scaler\n",
        "df_A_scaled = fun_scaler_minmax(df_A_scaled,['lat', 'lon', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
        "       'surface_covered'])\n",
        "df_A_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHu7azPHcEiQ"
      },
      "source": [
        "## **A. 05.** Generación de nuevas variables predictoras (reducción de dimensionalidad SVD/PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2CtvUFReZsE"
      },
      "source": [
        "Dado que queremos comparar los valores obtenidos por los modelos del primer proyecto con los valores obtenidos en este, y que los modelos que emplearon (regresor lineal, árbol de decisión y KNN) permitían conservar la interpretabilidad en relación con los features, no implementaremos SVD o PCA, pues aunque los resultados de los modelos puedan mejorar, nuestro enfoque es comparativo y no es útil perder la interpretabilidad en relación a las variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A0H2f6jeg02"
      },
      "source": [
        "## **A. 06.** Comparación resultados de los modelos (Proyecto 01 Vs Proyecto actual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi5P2NcnklmJ"
      },
      "source": [
        "Con el fin de evaluar cuál es la variación en las métricas de los modelos implementados en el primer proyecto PY_01 (regresor lineal, árbol de decisión y KNN) ser evaluaran los modelos bajo las mismas condiciones (iguales hiperparametros) y variando únicamente el dataset de entrada al cual hemos hecho todas las transformaciones y para el cual hemos incluido ahora variables adicionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALyKOICUlAc6"
      },
      "source": [
        "### ---> Definición de las nuevas variables predictoras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuy0FWUgQO8"
      },
      "source": [
        "X = df_A_scaled.drop('price', axis=1)\n",
        "y = df_A_scaled['price']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp_1tcM2lUf4"
      },
      "source": [
        "### ---> Train-Test Split (iguales parámetros PY_01)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCwK4tP3gSNA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_n0b0jAmAOm"
      },
      "source": [
        "### ---> Modelos instanciados (iguales hiperparametros PY_01)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLRzJVqGKLXP"
      },
      "source": [
        "linear_model = LinearRegression()\n",
        "tree_regressor = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1iQOp7cmcuL"
      },
      "source": [
        "### ---> Entrenamiento de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyTFT3ybKLZw"
      },
      "source": [
        "linear_model.fit(X_train, y_train)\n",
        "tree_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWIrxIldmpWJ"
      },
      "source": [
        "### ---> Resultados de los modelos (iguales hiperparámetros PY_01 - nuevo dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qH8oCJ3KLcD"
      },
      "source": [
        "modelos = ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos']\n",
        "\n",
        "for i, model in enumerate([linear_model, tree_regressor, knn_regressor]):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    \n",
        "    print(f'Modelo: {modelos[i]}')\n",
        "\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
        "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
        "    \n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    print(f'R cuadrado en Train: {r2_train}')\n",
        "    print(f'R cuadrado en Test: {r2_test}')\n",
        "    \n",
        "    plt.figure(figsize = (8,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.distplot(y_train - y_train_pred, bins = 50, label = 'train')\n",
        "    sns.distplot(y_test - y_test_pred, bins = 50, label = 'test')\n",
        "    plt.xlabel('errores')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    ax.scatter(y_test,y_test_pred, s =2)\n",
        "    \n",
        "    lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
        "    ]\n",
        "    \n",
        "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "    plt.xlabel('y (test)')\n",
        "    plt.ylabel('y_pred (test)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paxLbCn0g-wa"
      },
      "source": [
        "resultados_PY_01 = {'Modelo': ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos'], \n",
        "                    'RMSE_Train': [199173.83253528475, 191783.485243933, 121300.24049431273], \n",
        "                    'RMSE_Test': [202449.36540361575, 194688.15091192327, 155040.1332898459], \n",
        "                    'R2_Train': [0.5517887107017188, 0.5844333913686437, 0.8337576939918325],\n",
        "                    'R2_Test': [0.5514864358797202, 0.5852162030595955, 0.7369544202301519]\n",
        "                    }\n",
        "                    \n",
        "resultados_PY_02 = {'Modelo': ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos'], \n",
        "                    'RMSE_Train': [44143.814088767176, 54385.93455461426, 27331.957836355217], \n",
        "                    'RMSE_Test': [44214.72941966836, 54646.03060024996, 34287.79193709084], \n",
        "                    'R2_Train': [0.7699301113282079, 0.6507846978947374, 0.9118014270560322],\n",
        "                    'R2_Test': [0.7721363026987982, 0.6519365355152797, 0.8629685562592448]\n",
        "                    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twxgcblBFtRs"
      },
      "source": [
        "### ---> Evaluación comparativa entre los resultados del PY_01 y el proyecto actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmGVainXpc45"
      },
      "source": [
        "df_resultados_PY_01 = pd.DataFrame.from_dict(resultados_PY_01)\n",
        "df_resultados_PY_01['Proyecto']='PY_01'\n",
        "df_resultados_PY_01 = df_resultados_PY_01[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_01[['R2_Train', 'R2_Test']] = df_resultados_PY_01[['R2_Train', 'R2_Test']]\n",
        "\n",
        "df_resultados_PY_02 = pd.DataFrame.from_dict(resultados_PY_02)\n",
        "df_resultados_PY_02['Proyecto']='PY_02'\n",
        "df_resultados_PY_02 = df_resultados_PY_02[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_02[['R2_Train', 'R2_Test']] = df_resultados_PY_02[['R2_Train', 'R2_Test']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG9ISzEisZGC"
      },
      "source": [
        "df_resultados_compilados = pd.concat([df_resultados_PY_01,df_resultados_PY_02]).sort_values(by='Modelo', ascending=False).reset_index(drop=True)\n",
        "df_resultados_compilados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXcYj7F3siwT"
      },
      "source": [
        "list_delta_arbol = list(df_resultados_compilados.iloc[1, 2:6]-df_resultados_compilados.iloc[0, 2:6])\n",
        "list_delta_knn = list(df_resultados_compilados.iloc[3, 2:6]-df_resultados_compilados.iloc[2, 2:6])\n",
        "list_delta_lineal = list(df_resultados_compilados.iloc[5, 2:6]-df_resultados_compilados.iloc[4, 2:6])\n",
        "\n",
        "df_resultados_variacion = pd.DataFrame(list(zip(list_delta_arbol, list_delta_knn, list_delta_lineal)),\n",
        "               columns = ['Árbol de Decisión', 'Vecinos más cercanos', 'Regresión lineal']).T\n",
        "df_resultados_variacion.columns = ['Dif_RMSE_Train', 'Dif_RMSE_Test', 'Dif_R2_Train', 'Dif_R2_Test']              \n",
        "df_resultados_variacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsfbLoXlLLUh"
      },
      "source": [
        "En la presente sección hemos encontrado las variaciones que presentan  modelos de proyecto PY_01 (regresor lineal, árbol de decisión y KNN) únicamante a partir de la variación de dataset de entrada. En general todos los modelos mejoran sus métricas (RMSE y R2) tanto en el conjunto de test como en el conjunto de train. Los incrementos en el valor de R2 se dan en un rango entre un  6% y un 22%, con una mejora mucho más significativa en el modelo de regresión lineal.\n",
        "\n",
        "Puede observarse de igual manera que la mejora no se da únicamente en relación con las métricas, sino con la distribución de sus errores, presentando un comportamiento mucho más homocedástico en la nueva distribución. \n",
        "\n",
        "Como principales elementos en la contribución a la mejora en los resultados tanto de las métricas como de la distribución de sus errores podemos destacar los siguientes:\n",
        "- **Eliminación de outliers:** permite que el aprendizaje de los modelos se centre en los registros representativos de las distribuciones y no en los valores con comportamientos alejados de las mismas.\n",
        "- **Encoding:** permitió incluir en el modelo variables categóricas que antes no existían (los barrios específicamente que pueden ser una característica determinante en la estimación del modelo.\n",
        "- **Imputación de valores faltantes:** permitió por una parte prescindir de valores que podrían sesgar las estimaciones en el caso de ser imputados, y por otra parte imputar algunos que teniendo un sentido lógico en su estimación iban a enriquecer en cantidad de registros el dataset. Al tener más registros representativos el aprendizaje del modelo puede incrementar mejorando con ello sus métricas. \n",
        "- **Escalado de los valores:** permite al modelo tener un equilibrio en relación a las variables con respecto a las cuales hace su predicción haciendo una estimación mucho más homogénea de la importancia que toma cada una de las variables tanto en el aprendizaje como en la evaluación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0omlDNOLk72"
      },
      "source": [
        "# **Parte B.** Modelos Avanzados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wslxD8YksO"
      },
      "source": [
        "En la presente sección implementaremos algunos modelos complementarios basados en técnicas de ensamble, en particular Bagging y Boosting. En específico los modelos implementados son Random Forest Regressor y XGBoost. Para cada uno de ellos evaluaremos inicialmente los resultados que nos entregan con los hiperparametros asignados por defecto en cada una de sus librerías. De manera posterior haremos una optimización de sus hiperparámetros a través de métodos de grilla y evaluaremos el desempeño de los modelos obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceDMPacmxb1J"
      },
      "source": [
        "### ---> Función de evaluación para los modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RtiyuT4ZYQT"
      },
      "source": [
        "Con el fin de evaluar los modelos de manera homogénea definimos a continuación una función que nos permite calcular las métricas de R2 y RMSE, así como las gráficas de dispersión de errores para cada uno de los modelos. Emplearemos dicha función al final de cada sección para obtener los valores de rendimiento de cada modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATmY9k42xR3N"
      },
      "source": [
        "def evaluar_modelo(modelo, y_train, y_test, y_train_pred, y_test_pred):\n",
        "\n",
        "    print('El modelo a evaluar: ', modelo)\n",
        "    ### CALCULAMOS EL ERROR\n",
        "    \n",
        "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
        "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
        "    \n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    print(f'R cuadrado en Train: {r2_train}')\n",
        "    print(f'R cuadrado en Test: {r2_test}')\n",
        "\n",
        "    # print(f'Porcentaje del Error Absoluto medio en Train: {mape_train}')\n",
        "    # print(f'Porcentaje del Error Absoluto medio en Test: {mape_test}\\n')\n",
        "    \n",
        "        ### GRAFICAMOS LOS RESULTADOS\n",
        "      \n",
        "    plt.figure(figsize = (8,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.distplot(y_train - y_train_pred, bins = 20, label = 'train')\n",
        "    sns.distplot(y_test - y_test_pred, bins = 20, label = 'test')\n",
        "    plt.xlabel('errores')\n",
        "    plt.legend()\n",
        "\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    ax.scatter(y_test,y_test_pred, s =2)\n",
        "    \n",
        "    lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
        "    ]\n",
        "    \n",
        "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "    plt.xlabel('y (test)')\n",
        "    plt.ylabel('y_pred (test)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return rmse_train, rmse_test, r2_train, r2_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA6jaAfZweyg"
      },
      "source": [
        "## **B. 01.** Modelos avanzados (argumentos por defecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWy4aSFAYIDM"
      },
      "source": [
        "En la presente sección evaluaremos los modelos de random Forest Regressor y XGBoost con los hiperparametros que tienen asignados en su librerías por defecto. Los resultados serán evaluados de manera directa y también empleando la técnica de Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6dYFa1A_uzW"
      },
      "source": [
        "### **B.01.01** XGBoost (argumentos por defecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48YIMUUfwzPv"
      },
      "source": [
        "#### ----> Entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DK084QNFpP"
      },
      "source": [
        "# # Instanciamos el modelo con sus caracteristicas por defecto\n",
        "# XGB_model = xgb.XGBRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk7zvbZWNbRg"
      },
      "source": [
        "# # Corroboramos los hiperparametros que el modelo presenta\n",
        "# XGB_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOpsOGb6NFlt"
      },
      "source": [
        "# # Hacemos que el modelo se entrene con el set de entrenamiento (X_train, y_train)\n",
        "# # Caracteristicas: \n",
        "# # evalset : el modelo evaluara las métricas mape y rmse y se detendra tras 50 iteraciones\n",
        "# # en las que el modelo ho haya logrado obtener una metrica más baja. \n",
        "# # evalmetric: serán las metricas que emplearemos para evaluar lo pasos del del entrenamiento.\n",
        "# # La última de ellas es la que se constituye como criterio de parada para elearly stopping. \n",
        "# # early_stopping_rounds: cantidad de rondas que avanzará sin (si incluimos este parameto se almacenará el mejor modelo respecto a la metrica de evaluación)\n",
        "# # (Sino el modelo evaluará hasta el valor n_estimators y tomnara el valor del ultimo estimador)\n",
        "# # Verbose: Imprimirá cada una de los pasos del entrenamiento con sus respectivas evaluaciones sobre el eval_set\n",
        "# XGB_model.fit(X_train, y_train,\n",
        "#         eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "#         eval_metric= ['rmse'],\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=1\n",
        "#        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-MRMU928GJ"
      },
      "source": [
        "# Cargado del modelo guardado (una vez ya ha sido entrenado)\n",
        "path_XGB_model = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/XGB_model.sav'\n",
        "XGB_model = pickle.load(open(path_XGB_model, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1qbpwRmNFjQ"
      },
      "source": [
        "# Visualizamos el mejor valor para la métrica de criterio de entrenamiento\n",
        "XGB_model.best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRefRvKDNFhh"
      },
      "source": [
        "# Visualizamos cual fue la mejor iteración en la que dicho valor se obtuvo\n",
        "XGB_model.best_iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71KnOdUAScP6"
      },
      "source": [
        "# Generamos la predicción con el modelo entrenado\n",
        "y_train_pred = XGB_model.predict(X_train)\n",
        "y_test_pred= XGB_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYRbTcWNFcB"
      },
      "source": [
        "# Evaluamos los resultados obtenidos por el modelo\n",
        "rmse_train, rmse_test, r2_train, r2_test = evaluar_modelo('XGBoost (parametros por defecto)', y_train, y_test, y_train_pred, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxvpNFt2btrc"
      },
      "source": [
        "# Almacenamos los resultados obtenidos por el modelo\n",
        "resultados_PY_02_XGB_model = {'Modelo': ['XGBoost'], \n",
        "                              'RMSE_Train': [rmse_train], \n",
        "                              'RMSE_Test': [rmse_test], \n",
        "                              'R2_Train': [r2_train],\n",
        "                              'R2_Test': [r2_test]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47AWVTlq1WrX"
      },
      "source": [
        "# # Guardado del modelo ya entrenado\n",
        "# path_XGB_model = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/XGB_model.sav'\n",
        "# pickle.dump(XGB_model, open(path_XGB_model, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn-AZPX1_ebi"
      },
      "source": [
        "#### ---> Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H35JHlZ2dBr2"
      },
      "source": [
        "Mediante la técnica de Cross Validation corroboramos que el valor obtenido para las métricas es el valor representativo indiferente del conjunto de train y test seleccionado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8bwG6vh0XZ7"
      },
      "source": [
        "# CV_XGBoost_results = cross_validate(XGB_model, X_test, y_test, cv=5, scoring=['r2', 'neg_root_mean_squared_error'], return_train_score=True,  n_jobs=-1)\n",
        "# CV_XGBoost_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjAzIkX_8GlW"
      },
      "source": [
        "# CV_XGBoost_R2 = CV_XGBoost_results['test_r2']\n",
        "# CV_XGBoost_RMSE = CV_XGBoost_results['test_neg_root_mean_squared_error']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RawCqTRE0Xfn"
      },
      "source": [
        "# print('Modelo evaluado bajo Cross Validation:  XGBoost (parámetros por defecto)')\n",
        "# print(\"Raíz del error cuadrático medio para test : %0.2f (+/- %0.2f)\" % (abs(CV_XGBoost_RMSE.mean()), abs(CV_XGBoost_RMSE.std()) * 2))\n",
        "# print(\"R cuadrado para test : %0.2f (+/- %0.2f)\" % (abs(CV_XGBoost_R2.mean()), abs(CV_XGBoost_R2.std()) * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC2S9Iq-CvNO"
      },
      "source": [
        "### **B.01.02** Random Forest (argumentos por defecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G9vV41uCvNO"
      },
      "source": [
        "#### ----> Entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPvbYlrgCvNO"
      },
      "source": [
        "# # Instanciaremos el modelo con sus caracteristicas por defecto\n",
        "# RFR_Model = RandomForestRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nhWS7GxCvNP"
      },
      "source": [
        "# # Corroboramos los hiperparametros que el modelo presenta\n",
        "# RFR_Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StTqfaKKCvNP"
      },
      "source": [
        "# # Hacemos que el modelo se entrene con el set de entrenamiento (X_train, y_train)\n",
        "# RFR_Model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziu2f3M26OB0"
      },
      "source": [
        "# Cargado del modelo guardado (una vez ya ha sido entrenado)\n",
        "path_RFR_Model = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/RFR_Model.sav'\n",
        "RFR_Model = pickle.load(open(path_RFR_Model, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQfF0GEtCvNP"
      },
      "source": [
        "# Generamos la predicción con el modelo entrenado\n",
        "y_train_pred = RFR_Model.predict(X_train)\n",
        "y_test_pred= RFR_Model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxVqHnd0CvNQ"
      },
      "source": [
        "# Evaluamos los resultados obtenidos por el modelo\n",
        "rmse_train, rmse_test, r2_train, r2_test = evaluar_modelo('Random Forest Regressor (parametros por defecto)', y_train, y_test, y_train_pred, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FWXvXpgd0l6"
      },
      "source": [
        "# Almacenamos los resultados obtenidos por el modelo\n",
        "resultados_PY_02_RFR_Model = {'Modelo': ['Random Forest Regressor'], \n",
        "                              'RMSE_Train': [rmse_train], \n",
        "                              'RMSE_Test': [rmse_test], \n",
        "                              'R2_Train': [r2_train],\n",
        "                              'R2_Test': [r2_test]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ItwkLp36zjN"
      },
      "source": [
        "# # Guardado del modelo ya entrenado\n",
        "# path_RFR_Model = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/RFR_Model.sav'\n",
        "# pickle.dump(RFR_Model, open(path_RFR_Model, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75BLzhsZCvNQ"
      },
      "source": [
        "#### ---> Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiZZY4u-eQ5k"
      },
      "source": [
        "Mediante la técnica de Cross Validation corroboramos que el valor obtenido para las métricas es el valor representativo indiferente del conjunto de train y test seleccionado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lesRnYbCvNQ"
      },
      "source": [
        "# CV_RFR_Model_results = cross_validate(RFR_Model, X_test, y_test, cv=5, scoring=['r2', 'neg_root_mean_squared_error'], return_train_score=True,  n_jobs=-1)\n",
        "# CV_RFR_Model_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI2Er8_GCvNQ"
      },
      "source": [
        "# CV_RFR_Model_R2 = CV_RFR_Model_results['test_r2']\n",
        "# CV_RFR_Model_RMSE = CV_RFR_Model_results['test_neg_root_mean_squared_error']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWZqgdSVCvNQ"
      },
      "source": [
        "# print('Modelo evaluado bajo Cross Validation:  Random Forest Regressor (parametros por defecto)')\n",
        "# print(\"Raíz del error cuadrático medio : %0.2f (+/- %0.2f)\" % (abs(CV_RFR_Model_RMSE.mean()), abs(CV_RFR_Model_RMSE.std()) * 2))\n",
        "# print(\"R cuadrado : %0.2f (+/- %0.2f)\" % (abs(CV_RFR_Model_R2.mean()), abs(CV_RFR_Model_R2.std()) * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcoIbIcMFU9p"
      },
      "source": [
        "## **B. 02.** Modelos avanzados (hiperparámetros optimizados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epA8y4s3frO4"
      },
      "source": [
        "En la presente sección generaremos la optimización de hiperparámetros de los modelos Random Forest Regressor y XGBoost a partir de las técnicas de grilla, en específico Grid Search y Random Search. Los resultados serán evaluados empleando validación cruzada y se definirá el mejor modelo de cada tipo basado en los hiperparámetros optimizados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzmgBv64bAFC"
      },
      "source": [
        "### **B.02.01.** XGBoost (argumentos optimizados)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjwAgwd_NFZl"
      },
      "source": [
        "# # Definimos el modelo base a partir del cual haremos la optimización\n",
        "# XGB_model_base = xgb.XGBRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pldHyq9eNFXC"
      },
      "source": [
        "# # Definimos el conjunto de hiperparametros a partir del cual del cual harémos la optimización\n",
        "# param_XGB_model_base = {'nthread':[1, 2, 3, 4, 5], \n",
        "#               'objective':['reg:squarederror'],\n",
        "#               'learning_rate': np.arange(.05, 1, .05),\n",
        "#               'max_depth': np.arange(1,10),\n",
        "#               'min_child_weight': [3, 4, 5, 6, 7, 8],\n",
        "#               'subsample': np.arange(.05, 1, .05),\n",
        "#               'colsample_bytree': np.arange(.05, 1, .05),\n",
        "#               'n_estimators': [200, 300, 400, 500, 600]}\n",
        "# param_XGB_model_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThbB_CP9NFVj"
      },
      "source": [
        "# # A partir del modelo base y del conjunto de parametros definimos a través de una busqueda aleatoria con 50 iteraciones, \n",
        "# # los parametros a ser tenidos en cuanta para cada uno de los modelos.\n",
        "# XGB_model_RS = RandomizedSearchCV(XGB_model_base, param_XGB_model_base,n_iter=50, random_state=0, cv=5,\n",
        "#                             scoring='r2', verbose=True, n_jobs=-1, refit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFLRIrBXNFSK"
      },
      "source": [
        "# # Generamos el entrenamiento del conjunto de modelos definidos en la busqueda aleatoria\n",
        "# XGB_model_RS.fit(X_train, y_train,\n",
        "#         eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "#         early_stopping_rounds=50,\n",
        "#         verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxkkPEeQAy7o"
      },
      "source": [
        "# Cargado del modelo guardado (una vez ya ha sido entrenado)\n",
        "path_XGB_model_RS = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/XGB_model_RS.sav'\n",
        "XGB_model_RS = pickle.load(open(path_XGB_model_RS, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg2dgcjWNFQU"
      },
      "source": [
        "# Evaluamos cual fue el mejor resultado obtenido para la métrica definida\n",
        "XGB_model_RS.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KZLi-ZtNFO3"
      },
      "source": [
        "# Obtenemos los hiperparametros del modelo que presentó el mejor resultado para la métrica\n",
        "XGB_model_RS.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTcKiO5mu1WF"
      },
      "source": [
        "# Consultamos la iteración para la cual el modelo presentó el mejor resultado para la métrica\n",
        "best_score = np.argmax(XGB_model_RS.cv_results_['mean_test_score'])\n",
        "best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F03BO6VSNFLJ"
      },
      "source": [
        "# Imprimimos los resultados obtenidos para el mejor modelo así como los hiperparametros elegidos dela grilla \n",
        "print('mean_test_f1', XGB_model_RS.cv_results_['mean_test_score'][best_score])\n",
        "print('params', XGB_model_RS.cv_results_['params'][best_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lqIRuYlNFJI"
      },
      "source": [
        "# Generamos la predicción con el modelo que mejor métrica presento\n",
        "y_train_pred = XGB_model_RS.predict(X_train)\n",
        "y_test_pred= XGB_model_RS.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAykj2UNFF2"
      },
      "source": [
        "# Evaluamos los resultados obtenidos por el mejor modelo\n",
        "rmse_train, rmse_test, r2_train, r2_test = evaluar_modelo('XGBoost optimizado', y_train, y_test, y_train_pred, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDiZgT31l7ZQ"
      },
      "source": [
        "# Almacenamos los resultados obtenidos por el modelo\n",
        "resultados_PY_02_XGB_model_RS = {'Modelo': ['XGBoost optimizado'], \n",
        "                              'RMSE_Train': [rmse_train], \n",
        "                              'RMSE_Test': [rmse_test], \n",
        "                              'R2_Train': [r2_train],\n",
        "                              'R2_Test': [r2_test]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLpvhH3QqhkZ"
      },
      "source": [
        "# # Guardado del modelo ya entrenado\n",
        "# path_XGB_model_RS = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/XGB_model_RS.sav'\n",
        "# pickle.dump(XGB_model_RS, open(path_XGB_model_RS, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzUkjDpvIw5Q"
      },
      "source": [
        "### **B.02.02** Random Forest Regressor (argumentos optimizados)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhiHc2luIw5R"
      },
      "source": [
        "# # Definimos el modelo base a partir del cual haremos la optimización\n",
        "# RFR_model_base = RandomForestRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHl-GHuQIw5R"
      },
      "source": [
        "# # Definimos el conjunto de hiperparametros a partir del cual del cual harémos la optimización\n",
        "# param_RFR_model_base = {'bootstrap': [True, False],\n",
        "#             'max_depth': [10, 100],\n",
        "#             'max_features': ['auto', 'sqrt'],\n",
        "#             'n_estimators': [200, 1000]}\n",
        "# param_RFR_model_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqd8e_LHIw5S"
      },
      "source": [
        "# # A partir del modelo base y del conjunto de parametros definimos a través de una busqueda por grilla \n",
        "# # los parametros a ser tenidos en cuanta para cada uno de los modelos.\n",
        "# RFR_model_RS = GridSearchCV(RFR_model_base, param_RFR_model_base, cv=5,\n",
        "#                             scoring='r2', verbose=True, n_jobs=-1, refit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X088DZsIw5S"
      },
      "source": [
        "# # Generamos el entrenamiento del conjunto de modelos definidos en la busqueda por grilla\n",
        "# RFR_model_RS.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LoKT80MDlrk"
      },
      "source": [
        "# Cargado del modelo guardado (una vez ya ha sido entrenado)\n",
        "path_RFR_model_RS = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/RFR_model_RS.sav'\n",
        "RFR_model_RS = pickle.load(open(path_RFR_model_RS, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmfNbF13YNO6"
      },
      "source": [
        "# Evaluamos cual fue el mejor resultado obtenido para la métrica definida\n",
        "RFR_model_RS.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMRm7AiSpVK2"
      },
      "source": [
        "# Evaluamos cual fue el mejor resultado obtenido para la métrica definida\n",
        "RFR_model_RS.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yl5pzVWpVK3"
      },
      "source": [
        "# Obtenemos los hiperparametros del modelo que presentó el mejor resultado para la métrica\n",
        "RFR_model_RS.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9UIlwHTpVK3"
      },
      "source": [
        "# Consultamos la iteración para la cual el modelo presentó el mejor resultado para la métrica\n",
        "best_score = np.argmax(RFR_model_RS.cv_results_['mean_test_score'])\n",
        "best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5d3CC8VpVK3"
      },
      "source": [
        "# Imprimimos los resultados obtenidos para el mejor modelo así como los hiperparametros elegidos dela grilla \n",
        "print('mean_test_R2', RFR_model_RS.cv_results_['mean_test_score'][best_score])\n",
        "print('params', RFR_model_RS.cv_results_['params'][best_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaFgQ4MNpVK4"
      },
      "source": [
        "# Generamos la predicción con el modelo que mejor métrica presento\n",
        "y_train_pred = RFR_model_RS.predict(X_train)\n",
        "y_test_pred= RFR_model_RS.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3D_uDe8pVK4"
      },
      "source": [
        "# Evaluamos los resultados obtenidos por el mejor modelo\n",
        "rmse_train, rmse_test, r2_train, r2_test = evaluar_modelo('Random Forest Regressor optimizado', y_train, y_test, y_train_pred, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je2SDJ9YpVK4"
      },
      "source": [
        "# Almacenamos los resultados obtenidos por el modelo\n",
        "resultados_PY_02_RFR_model_RS = {'Modelo': ['Random Forest Regressor optimizado'], \n",
        "                              'RMSE_Train': [rmse_train], \n",
        "                              'RMSE_Test': [rmse_test], \n",
        "                              'R2_Train': [r2_train],\n",
        "                              'R2_Test': [r2_test]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6GBOacAmZgd"
      },
      "source": [
        "# # Guardado del modelo ya entrenado\n",
        "# path_RFR_model_RS = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/RFR_model_RS.sav'\n",
        "# pickle.dump(RFR_model_RS, open(path_RFR_model_RS, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT8W7GsETYXZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi94-Rp-TZH0"
      },
      "source": [
        "## **B. 03.** Evaluación de desempeño"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bUNPMH0TtOS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXp7epKjTtRe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5p2UDn4TtUP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9CwwCYaTtWe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPvyQhSXTtab"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igprzki7TtdC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxz2o1YBTtfA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0dP-5xHTth-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YioBJO0pmfmc"
      },
      "source": [
        "# POPURRI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crhcxGciTRBl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVlKoXwETRVk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxBnveUsNih"
      },
      "source": [
        "resultados_PY_01 = {'Modelo': ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos'], \n",
        "                    'RMSE_Train': [199173.83253528475, 191783.485243933, 121300.24049431273], \n",
        "                    'RMSE_Test': [202449.36540361575, 194688.15091192327, 155040.1332898459], \n",
        "                    'R2_Train': [0.5517887107017188, 0.5844333913686437, 0.8337576939918325],\n",
        "                    'R2_Test': [0.5514864358797202, 0.5852162030595955, 0.7369544202301519]\n",
        "                    }\n",
        "                    \n",
        "resultados_PY_02 = {'Modelo': ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos'], \n",
        "                    'RMSE_Train': [44143.814088767176, 54385.93455461426, 27331.957836355217], \n",
        "                    'RMSE_Test': [44214.72941966836, 54646.03060024996, 34287.79193709084], \n",
        "                    'R2_Train': [0.7699301113282079, 0.6507846978947374, 0.9118014270560322],\n",
        "                    'R2_Test': [0.7721363026987982, 0.6519365355152797, 0.8629685562592448]\n",
        "                    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IcA-EMetgW0"
      },
      "source": [
        "resultados_PY_02_XGB_model = {'Modelo': ['XGBoost'], \n",
        "                              'RMSE_Train': [28005.652518335588], \n",
        "                              'RMSE_Test': [32175.555797713354], \n",
        "                              'R2_Train': [0.9073998977854276],\n",
        "                              'R2_Test': [0.8793316673319217]\n",
        "                              }\n",
        "\n",
        "resultados_PY_02_RFR_Model = {'Modelo': ['Random Forest Regressor'], \n",
        "                              'RMSE_Train': [10963.78330981362], \n",
        "                              'RMSE_Test': [26194.483657392666], \n",
        "                              'R2_Train': [0.9858080863167111],\n",
        "                              'R2_Test': [0.9200237786825292]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1m8q94RsN1N"
      },
      "source": [
        "resultados_PY_02_XGB_model_RS = {'Modelo': ['XGBoost optimizado'], \n",
        "                              'RMSE_Train': [1.652518335588], \n",
        "                              'RMSE_Test': [1.555797713354], \n",
        "                              'R2_Train': [1],\n",
        "                              'R2_Test': [1]\n",
        "                              }\n",
        "resultados_PY_02_RFR_model_RS = {'Modelo': ['Random Forest Regressor optimizado'], \n",
        "                              'RMSE_Train': [2.652518335588], \n",
        "                              'RMSE_Test': [2.555797713354], \n",
        "                              'R2_Train': [2],\n",
        "                              'R2_Test': [2]\n",
        "                              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKSH2q8sN35"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlflS8Ysfv1"
      },
      "source": [
        "df_resultados_PY_01 = pd.DataFrame.from_dict(resultados_PY_01)\n",
        "df_resultados_PY_01['Proyecto']='PY_01'\n",
        "df_resultados_PY_01 = df_resultados_PY_01[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_01[['R2_Train', 'R2_Test']] = 100*df_resultados_PY_01[['R2_Train', 'R2_Test']]\n",
        "\n",
        "df_resultados_PY_02 = pd.DataFrame.from_dict(resultados_PY_02)\n",
        "df_resultados_PY_02['Proyecto']='PY_02'\n",
        "df_resultados_PY_02 = df_resultados_PY_02[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_02[['R2_Train', 'R2_Test']] = 100*df_resultados_PY_02[['R2_Train', 'R2_Test']]\n",
        "\n",
        "df_resultados_PY_02_XGB_model = pd.DataFrame.from_dict(resultados_PY_02_XGB_model)\n",
        "df_resultados_PY_02_XGB_model['Proyecto']='PY_02'\n",
        "df_resultados_PY_02_XGB_model = df_resultados_PY_02_XGB_model[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_02_XGB_model[['R2_Train', 'R2_Test']] = 100*df_resultados_PY_02_XGB_model[['R2_Train', 'R2_Test']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_resultados_PY_02_XGB_model_RS = pd.DataFrame.from_dict(resultados_PY_02_XGB_model_RS)\n",
        "df_resultados_PY_02_XGB_model_RS['Proyecto']='PY_02'\n",
        "df_resultados_PY_02_XGB_model_RS = df_resultados_PY_02_XGB_model_RS[['Modelo', 'Proyecto', 'RMSE_Train', 'RMSE_Test', 'R2_Train', 'R2_Test']]\n",
        "df_resultados_PY_02_XGB_model_RS[['R2_Train', 'R2_Test']] = 100*df_resultados_PY_02_XGB_model_RS[['R2_Train', 'R2_Test']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX9B7kLSsfv2"
      },
      "source": [
        "df_resultados_compilados = pd.concat([df_resultados_PY_01,df_resultados_PY_02,df_resultados_PY_02_XGB_model ]).sort_values(by='Modelo', ascending=False).reset_index(drop=True)\n",
        "df_resultados_compilados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYOHgJfmmZlV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2dbi5yjmZnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNzx4e5RmZpz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAG5qzDWIw5V"
      },
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "plot_importance(xgb_rs.best_estimator_, height=0.9,max_num_features=15)\n",
        "plt.title('Predicción con modelo XGBoost (Múltiples parametros optimizado) Feature importance',fontsize=16)\n",
        "plt.xlabel('Feature', fontsize=15)\n",
        "plt.ylabel('F score', fontsize=15)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVzq3147NE9g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK1xcRt-NE7F"
      },
      "source": [
        "import dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILK09lTSi-81"
      },
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/00_Acamica/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XqWk39li-_h"
      },
      "source": [
        "dill.dump_session(checkpoint_path+'02_salida_env.db')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8qDf4qZi_CH"
      },
      "source": [
        "dill.load_session(checkpoint_path+'02_salida_env.db')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIehbr9ji_EV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMunSAQi_Gb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBe6alKli_J2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR9w7Dgdi_Zr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQjwglhkjU3p"
      },
      "source": [
        "# Parte C. Interpretación de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl__eTUMjCla"
      },
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "plot_importance(XGB_model_RS.best_estimator_, height=0.9,max_num_features=15)\n",
        "plt.title('Predicción con modelo XGBoost (Múltiples parametros optimizado) Feature importance',fontsize=16)\n",
        "plt.xlabel('Feature', fontsize=15)\n",
        "plt.ylabel('F score', fontsize=15)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}